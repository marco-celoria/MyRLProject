{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f3843",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1660990073224,
     "user": {
      "displayName": "Marco Celoria",
      "userId": "03153579951134764980"
     },
     "user_tz": -120
    },
    "id": "411f3843",
    "outputId": "b8c97f2e-5290-4e37-fecf-f8bdb1d01969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'multiagent-particle-envs'...\n",
      "remote: Enumerating objects: 242, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 242 (delta 0), reused 3 (delta 0), pack-reused 237\u001b[K\n",
      "Receiving objects: 100% (242/242), 107.24 KiB | 11.92 MiB/s, done.\n",
      "Resolving deltas: 100% (127/127), done.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients\n",
    "\n",
    "!git clone https://github.com/openai/multiagent-particle-envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf42c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1660990073227,
     "user": {
      "displayName": "Marco Celoria",
      "userId": "03153579951134764980"
     },
     "user_tz": -120
    },
    "id": "21bf42c0",
    "outputId": "d5f207b3-dfb0-43dd-ed0c-81aec7f33c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/multiagent-particle-envs\n"
     ]
    }
   ],
   "source": [
    "cd multiagent-particle-envs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Puq-9QtTk8T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 121690,
     "status": "ok",
     "timestamp": 1660990194906,
     "user": {
      "displayName": "Marco Celoria",
      "userId": "03153579951134764980"
     },
     "user_tz": -120
    },
    "id": "9Puq-9QtTk8T",
    "outputId": "78c03a75-e787-48f8-ddfe-4da9f6c6832b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gym==0.10.5\n",
      "  Downloading gym-0.10.5.tar.gz (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 7.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.21.6)\n",
      "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n",
      "Collecting pyglet>=1.2.0\n",
      "  Downloading pyglet-1.5.26-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 34.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581307 sha256=519b8eb9301a8a8a61c2ae9a6833afa1e4a6f88dae101f682e67b52bc1e9f613\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.25.1\n",
      "    Uninstalling gym-0.25.1:\n",
      "      Successfully uninstalled gym-0.25.1\n",
      "Successfully installed gym-0.10.5 pyglet-1.5.26\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch==1.4.0\n",
      "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4 MB 7.1 kB/s \n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu113\n",
      "    Uninstalling torch-1.12.1+cu113:\n",
      "      Successfully uninstalled torch-1.12.1+cu113\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
      "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
      "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.4.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting numpy==1.14.5\n",
      "  Downloading numpy-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 7.9 MB/s \n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.5 which is incompatible.\n",
      "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.5 which is incompatible.\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.14.5 which is incompatible.\n",
      "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
      "tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.5 which is incompatible.\n",
      "thinc 8.1.0 requires numpy>=1.15.0, but you have numpy 1.14.5 which is incompatible.\n",
      "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.14.5 which is incompatible.\n",
      "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.5 which is incompatible.\n",
      "statsmodels 0.12.2 requires numpy>=1.15, but you have numpy 1.14.5 which is incompatible.\n",
      "spacy 3.4.1 requires numpy>=1.15.0, but you have numpy 1.14.5 which is incompatible.\n",
      "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.5 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.14.5 which is incompatible.\n",
      "scikit-learn 1.0.2 requires numpy>=1.14.6, but you have numpy 1.14.5 which is incompatible.\n",
      "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.5 which is incompatible.\n",
      "resampy 0.4.0 requires numpy>=1.17, but you have numpy 1.14.5 which is incompatible.\n",
      "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.5 which is incompatible.\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.14.5 which is incompatible.\n",
      "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.5 which is incompatible.\n",
      "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.5 which is incompatible.\n",
      "prophet 1.1 requires numpy>=1.15.4, but you have numpy 1.14.5 which is incompatible.\n",
      "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.14.5 which is incompatible.\n",
      "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.5 which is incompatible.\n",
      "numba 0.56.0 requires numpy<1.23,>=1.18, but you have numpy 1.14.5 which is incompatible.\n",
      "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.5 which is incompatible.\n",
      "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.5 which is incompatible.\n",
      "jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.5 which is incompatible.\n",
      "jax 0.3.14 requires numpy>=1.19, but you have numpy 1.14.5 which is incompatible.\n",
      "imgaug 0.4.0 requires numpy>=1.15, but you have numpy 1.14.5 which is incompatible.\n",
      "httpstan 4.6.1 requires numpy<2.0,>=1.16, but you have numpy 1.14.5 which is incompatible.\n",
      "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
      "cvxpy 1.2.1 requires numpy>=1.15, but you have numpy 1.14.5 which is incompatible.\n",
      "cupy-cuda111 9.4.0 requires numpy<1.24,>=1.17, but you have numpy 1.14.5 which is incompatible.\n",
      "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.14.5 which is incompatible.\n",
      "blis 0.7.8 requires numpy>=1.15.0, but you have numpy 1.14.5 which is incompatible.\n",
      "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.14.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Obtaining file:///content/multiagent-particle-envs\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from multiagent==0.0.1) (0.10.5)\n",
      "Collecting numpy-stl\n",
      "  Downloading numpy_stl-2.17.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.5.26)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.14.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from numpy-stl->multiagent==0.0.1) (3.3.3)\n",
      "Installing collected packages: numpy-stl, multiagent\n",
      "  Running setup.py develop for multiagent\n",
      "Successfully installed multiagent-0.0.1 numpy-stl-2.17.1\n",
      "Package                       Version                      Location\n",
      "----------------------------- ---------------------------- ---------------------------------\n",
      "absl-py                       1.2.0\n",
      "aiohttp                       3.8.1\n",
      "aiosignal                     1.2.0\n",
      "alabaster                     0.7.12\n",
      "albumentations                1.2.1\n",
      "altair                        4.2.0\n",
      "appdirs                       1.4.4\n",
      "arviz                         0.12.1\n",
      "astor                         0.8.1\n",
      "astropy                       4.3.1\n",
      "astunparse                    1.6.3\n",
      "async-timeout                 4.0.2\n",
      "asynctest                     0.13.0\n",
      "atari-py                      0.2.9\n",
      "atomicwrites                  1.4.1\n",
      "attrs                         22.1.0\n",
      "audioread                     3.0.0\n",
      "autograd                      1.4\n",
      "Babel                         2.10.3\n",
      "backcall                      0.2.0\n",
      "beautifulsoup4                4.6.3\n",
      "bleach                        5.0.1\n",
      "blis                          0.7.8\n",
      "bokeh                         2.3.3\n",
      "branca                        0.5.0\n",
      "bs4                           0.0.1\n",
      "CacheControl                  0.12.11\n",
      "cached-property               1.5.2\n",
      "cachetools                    4.2.4\n",
      "catalogue                     2.0.8\n",
      "certifi                       2022.6.15\n",
      "cffi                          1.15.1\n",
      "cftime                        1.6.1\n",
      "chardet                       3.0.4\n",
      "charset-normalizer            2.1.0\n",
      "click                         7.1.2\n",
      "clikit                        0.6.2\n",
      "cloudpickle                   1.5.0\n",
      "cmake                         3.22.6\n",
      "cmdstanpy                     1.0.4\n",
      "colorcet                      3.0.0\n",
      "colorlover                    0.3.0\n",
      "community                     1.0.0b1\n",
      "contextlib2                   0.5.5\n",
      "convertdate                   2.4.0\n",
      "crashtest                     0.3.1\n",
      "crcmod                        1.7\n",
      "cufflinks                     0.17.3\n",
      "cupy-cuda111                  9.4.0\n",
      "cvxopt                        1.3.0\n",
      "cvxpy                         1.2.1\n",
      "cycler                        0.11.0\n",
      "cymem                         2.0.6\n",
      "Cython                        0.29.32\n",
      "daft                          0.0.4\n",
      "dask                          2022.2.0\n",
      "datascience                   0.17.5\n",
      "debugpy                       1.0.0\n",
      "decorator                     4.4.2\n",
      "defusedxml                    0.7.1\n",
      "deprecat                      2.1.1\n",
      "descartes                     1.1.0\n",
      "dill                          0.3.5.1\n",
      "distributed                   2022.2.0\n",
      "dlib                          19.24.0\n",
      "dm-tree                       0.1.7\n",
      "docutils                      0.17.1\n",
      "dopamine-rl                   1.0.5\n",
      "earthengine-api               0.1.319\n",
      "easydict                      1.9\n",
      "ecos                          2.0.10\n",
      "editdistance                  0.5.3\n",
      "en-core-web-sm                3.4.0\n",
      "entrypoints                   0.4\n",
      "ephem                         4.1.3\n",
      "et-xmlfile                    1.1.0\n",
      "etils                         0.7.1\n",
      "fa2                           0.3.5\n",
      "fastai                        2.7.9\n",
      "fastcore                      1.5.18\n",
      "fastdownload                  0.0.7\n",
      "fastdtw                       0.3.4\n",
      "fastjsonschema                2.16.1\n",
      "fastprogress                  1.0.3\n",
      "fastrlock                     0.8\n",
      "feather-format                0.4.1\n",
      "filelock                      3.8.0\n",
      "firebase-admin                4.4.0\n",
      "fix-yahoo-finance             0.0.22\n",
      "Flask                         1.1.4\n",
      "flatbuffers                   2.0\n",
      "folium                        0.12.1.post1\n",
      "frozenlist                    1.3.1\n",
      "fsspec                        2022.7.1\n",
      "future                        0.16.0\n",
      "gast                          0.5.3\n",
      "GDAL                          2.2.2\n",
      "gdown                         4.4.0\n",
      "gensim                        3.6.0\n",
      "geographiclib                 1.52\n",
      "geopy                         1.17.0\n",
      "gin-config                    0.5.0\n",
      "glob2                         0.7\n",
      "google                        2.0.3\n",
      "google-api-core               1.31.6\n",
      "google-api-python-client      1.12.11\n",
      "google-auth                   1.35.0\n",
      "google-auth-httplib2          0.0.4\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-cloud-bigquery         1.21.0\n",
      "google-cloud-bigquery-storage 1.1.2\n",
      "google-cloud-core             1.0.3\n",
      "google-cloud-datastore        1.8.0\n",
      "google-cloud-firestore        1.7.0\n",
      "google-cloud-language         1.2.0\n",
      "google-cloud-storage          1.18.1\n",
      "google-cloud-translate        1.5.0\n",
      "google-colab                  1.0.0\n",
      "google-pasta                  0.2.0\n",
      "google-resumable-media        0.4.1\n",
      "googleapis-common-protos      1.56.4\n",
      "googledrivedownloader         0.4\n",
      "graphviz                      0.10.1\n",
      "greenlet                      1.1.2\n",
      "grpcio                        1.47.0\n",
      "gspread                       3.4.2\n",
      "gspread-dataframe             3.0.8\n",
      "gym                           0.10.5\n",
      "gym-notices                   0.0.8\n",
      "h5py                          3.1.0\n",
      "HeapDict                      1.0.1\n",
      "hijri-converter               2.2.4\n",
      "holidays                      0.14.2\n",
      "holoviews                     1.14.9\n",
      "html5lib                      1.0.1\n",
      "httpimport                    0.5.18\n",
      "httplib2                      0.17.4\n",
      "httplib2shim                  0.0.3\n",
      "httpstan                      4.6.1\n",
      "humanize                      0.5.1\n",
      "hyperopt                      0.1.2\n",
      "idna                          2.10\n",
      "imageio                       2.9.0\n",
      "imagesize                     1.4.1\n",
      "imbalanced-learn              0.8.1\n",
      "imblearn                      0.0\n",
      "imgaug                        0.4.0\n",
      "importlib-metadata            4.12.0\n",
      "importlib-resources           5.9.0\n",
      "imutils                       0.5.4\n",
      "inflect                       2.1.0\n",
      "intel-openmp                  2022.1.0\n",
      "intervaltree                  2.1.0\n",
      "ipykernel                     5.3.4\n",
      "ipython                       7.9.0\n",
      "ipython-genutils              0.2.0\n",
      "ipython-sql                   0.3.9\n",
      "ipywidgets                    8.0.1\n",
      "itsdangerous                  1.1.0\n",
      "jax                           0.3.14\n",
      "jaxlib                        0.3.14+cuda11.cudnn805\n",
      "jieba                         0.42.1\n",
      "Jinja2                        2.11.3\n",
      "joblib                        1.1.0\n",
      "jpeg4py                       0.1.4\n",
      "jsonschema                    4.3.3\n",
      "jupyter-client                6.1.12\n",
      "jupyter-console               6.1.0\n",
      "jupyter-core                  4.11.1\n",
      "jupyterlab-widgets            3.0.1\n",
      "kaggle                        1.5.12\n",
      "kapre                         0.3.7\n",
      "keras                         2.8.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "keras-vis                     0.4.1\n",
      "kiwisolver                    1.4.4\n",
      "korean-lunar-calendar         0.2.1\n",
      "langcodes                     3.3.0\n",
      "libclang                      14.0.6\n",
      "librosa                       0.8.1\n",
      "lightgbm                      2.2.3\n",
      "llvmlite                      0.39.0\n",
      "lmdb                          0.99\n",
      "locket                        1.0.0\n",
      "LunarCalendar                 0.0.9\n",
      "lxml                          4.9.1\n",
      "Markdown                      3.4.1\n",
      "MarkupSafe                    2.0.1\n",
      "marshmallow                   3.17.0\n",
      "matplotlib                    3.2.2\n",
      "matplotlib-venn               0.11.7\n",
      "missingno                     0.5.1\n",
      "mistune                       0.8.4\n",
      "mizani                        0.7.3\n",
      "mkl                           2019.0\n",
      "mlxtend                       0.14.0\n",
      "more-itertools                8.14.0\n",
      "moviepy                       0.2.3.5\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.4\n",
      "multiagent                    0.0.1                        /content/multiagent-particle-envs\n",
      "multidict                     6.0.2\n",
      "multitasking                  0.0.11\n",
      "murmurhash                    1.0.8\n",
      "music21                       5.5.0\n",
      "natsort                       5.5.0\n",
      "nbconvert                     5.6.1\n",
      "nbformat                      5.4.0\n",
      "netCDF4                       1.6.0\n",
      "networkx                      2.6.3\n",
      "nibabel                       3.0.2\n",
      "nltk                          3.7\n",
      "notebook                      5.3.1\n",
      "numba                         0.56.0\n",
      "numexpr                       2.8.3\n",
      "numpy                         1.14.5\n",
      "numpy-stl                     2.17.1\n",
      "oauth2client                  4.1.3\n",
      "oauthlib                      3.2.0\n",
      "okgrade                       0.4.3\n",
      "opencv-contrib-python         4.6.0.66\n",
      "opencv-python                 4.6.0.66\n",
      "opencv-python-headless        4.6.0.66\n",
      "openpyxl                      3.0.10\n",
      "opt-einsum                    3.3.0\n",
      "osqp                          0.6.2.post0\n",
      "packaging                     21.3\n",
      "palettable                    3.3.0\n",
      "pandas                        1.3.5\n",
      "pandas-datareader             0.9.0\n",
      "pandas-gbq                    0.13.3\n",
      "pandas-profiling              1.4.1\n",
      "pandocfilters                 1.5.0\n",
      "panel                         0.12.1\n",
      "param                         1.12.2\n",
      "parso                         0.8.3\n",
      "partd                         1.3.0\n",
      "pastel                        0.2.1\n",
      "pathlib                       1.0.1\n",
      "pathy                         0.6.2\n",
      "patsy                         0.5.2\n",
      "pep517                        0.13.0\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        7.1.2\n",
      "pip                           21.1.3\n",
      "pip-tools                     6.2.0\n",
      "plotly                        5.5.0\n",
      "plotnine                      0.8.0\n",
      "pluggy                        0.7.1\n",
      "pooch                         1.6.0\n",
      "portpicker                    1.3.9\n",
      "prefetch-generator            1.0.1\n",
      "preshed                       3.0.7\n",
      "prettytable                   3.3.0\n",
      "progressbar2                  3.38.0\n",
      "promise                       2.3\n",
      "prompt-toolkit                2.0.10\n",
      "prophet                       1.1\n",
      "protobuf                      3.17.3\n",
      "psutil                        5.4.8\n",
      "psycopg2                      2.9.3\n",
      "ptyprocess                    0.7.0\n",
      "py                            1.11.0\n",
      "pyarrow                       6.0.1\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycocotools                   2.0.4\n",
      "pycparser                     2.21\n",
      "pyct                          0.4.8\n",
      "pydantic                      1.9.2\n",
      "pydata-google-auth            1.4.0\n",
      "pydot                         1.3.0\n",
      "pydot-ng                      2.0.0\n",
      "pydotplus                     2.0.2\n",
      "PyDrive                       1.3.1\n",
      "pyemd                         0.5.1\n",
      "pyerfa                        2.0.0.1\n",
      "pyglet                        1.5.26\n",
      "Pygments                      2.6.1\n",
      "pygobject                     3.26.1\n",
      "pylev                         1.4.0\n",
      "pymc3                         3.11.5\n",
      "PyMeeus                       0.5.11\n",
      "pymongo                       4.2.0\n",
      "pymystem3                     0.2.0\n",
      "PyOpenGL                      3.1.6\n",
      "pyparsing                     3.0.9\n",
      "pyrsistent                    0.18.1\n",
      "pysimdjson                    3.2.0\n",
      "pysndfile                     1.3.8\n",
      "PySocks                       1.7.1\n",
      "pystan                        3.3.0\n",
      "pytest                        3.6.4\n",
      "python-apt                    0.0.0\n",
      "python-chess                  0.23.11\n",
      "python-dateutil               2.8.2\n",
      "python-louvain                0.16\n",
      "python-slugify                6.1.2\n",
      "python-utils                  3.3.3\n",
      "pytz                          2022.2.1\n",
      "pyviz-comms                   2.2.0\n",
      "PyWavelets                    1.3.0\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.1\n",
      "qdldl                         0.1.5.post2\n",
      "qudida                        0.0.4\n",
      "regex                         2022.6.2\n",
      "requests                      2.23.0\n",
      "requests-oauthlib             1.3.1\n",
      "resampy                       0.4.0\n",
      "rpy2                          3.4.5\n",
      "rsa                           4.9\n",
      "scikit-image                  0.18.3\n",
      "scikit-learn                  1.0.2\n",
      "scipy                         1.7.3\n",
      "screen-resolution-extra       0.0.0\n",
      "scs                           3.2.0\n",
      "seaborn                       0.11.2\n",
      "semver                        2.13.0\n",
      "Send2Trash                    1.8.0\n",
      "setuptools                    57.4.0\n",
      "setuptools-git                1.2\n",
      "Shapely                       1.8.4\n",
      "six                           1.15.0\n",
      "sklearn-pandas                1.8.0\n",
      "smart-open                    5.2.1\n",
      "snowballstemmer               2.2.0\n",
      "sortedcontainers              2.4.0\n",
      "SoundFile                     0.10.3.post1\n",
      "spacy                         3.4.1\n",
      "spacy-legacy                  3.0.10\n",
      "spacy-loggers                 1.0.3\n",
      "Sphinx                        1.8.6\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "sphinxcontrib-websupport      1.2.4\n",
      "SQLAlchemy                    1.4.40\n",
      "sqlparse                      0.4.2\n",
      "srsly                         2.4.4\n",
      "statsmodels                   0.12.2\n",
      "sympy                         1.7.1\n",
      "tables                        3.7.0\n",
      "tabulate                      0.8.10\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.0.1\n",
      "tensorboard                   2.8.0\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorflow                    2.8.2+zzzcolab20220719082949\n",
      "tensorflow-datasets           4.6.0\n",
      "tensorflow-estimator          2.8.0\n",
      "tensorflow-gcs-config         2.8.0\n",
      "tensorflow-hub                0.12.0\n",
      "tensorflow-io-gcs-filesystem  0.26.0\n",
      "tensorflow-metadata           1.9.0\n",
      "tensorflow-probability        0.16.0\n",
      "termcolor                     1.1.0\n",
      "terminado                     0.13.3\n",
      "testpath                      0.6.0\n",
      "text-unidecode                1.3\n",
      "textblob                      0.15.3\n",
      "Theano-PyMC                   1.1.2\n",
      "thinc                         8.1.0\n",
      "threadpoolctl                 3.1.0\n",
      "tifffile                      2021.11.2\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "toolz                         0.12.0\n",
      "torch                         1.4.0\n",
      "torchaudio                    0.12.1+cu113\n",
      "torchsummary                  1.5.1\n",
      "torchtext                     0.13.1\n",
      "torchvision                   0.13.1+cu113\n",
      "tornado                       5.1.1\n",
      "tqdm                          4.64.0\n",
      "traitlets                     5.1.1\n",
      "tweepy                        3.10.0\n",
      "typeguard                     2.7.1\n",
      "typer                         0.4.2\n",
      "typing-extensions             4.1.1\n",
      "tzlocal                       1.5.1\n",
      "ujson                         5.4.0\n",
      "uritemplate                   3.0.1\n",
      "urllib3                       1.24.3\n",
      "vega-datasets                 0.9.0\n",
      "wasabi                        0.10.1\n",
      "wcwidth                       0.2.5\n",
      "webargs                       8.2.0\n",
      "webencodings                  0.5.1\n",
      "Werkzeug                      1.0.1\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            4.0.1\n",
      "wordcloud                     1.8.2.2\n",
      "wrapt                         1.14.1\n",
      "xarray                        0.20.2\n",
      "xarray-einstats               0.2.2\n",
      "xgboost                       0.90\n",
      "xkit                          0.0.0\n",
      "xlrd                          1.1.0\n",
      "xlwt                          1.3.0\n",
      "yarl                          1.8.1\n",
      "yellowbrick                   1.4\n",
      "zict                          2.2.0\n",
      "zipp                          3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.10.5\n",
    "!pip install torch==1.4.0\n",
    "!pip install numpy==1.14.5\n",
    "!pip install -e .\n",
    "\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kH-Cj6bVT0p0",
   "metadata": {
    "id": "kH-Cj6bVT0p0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from make_env import make_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec0423",
   "metadata": {},
   "source": [
    "Let's inspect the environment: \n",
    "- There are three agents\n",
    "- The observation space is Box(8,) for the adversary red agent\n",
    "- The observation space is Box(10,) for the two collaborative green agents\n",
    "- The possible actions are 5: don't move, move up,down,left,right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jolo64lGT3AZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1660990195396,
     "user": {
      "displayName": "Marco Celoria",
      "userId": "03153579951134764980"
     },
     "user_tz": -120
    },
    "id": "Jolo64lGT3AZ",
    "outputId": "15957aef-edfb-4190-d0a4-0ad334cc96b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of agents: 3\n",
      "observation space: [Box(8,), Box(10,), Box(10,)]\n",
      "action space: [Discrete(5), Discrete(5), Discrete(5)]\n",
      "n actions 5\n"
     ]
    }
   ],
   "source": [
    "env = make_env(\"simple_adversary\")\n",
    "print(\"number of agents:\", env.n)\n",
    "print(\"observation space:\", env.observation_space)\n",
    "print(\"action space:\", env.action_space)\n",
    "print(\"n actions\", env.action_space[0].n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39520e19",
   "metadata": {},
   "source": [
    "Actually, the obervation space is basically a list of three arrays of 8, 10 and 10 double each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BaozE79pT8Jl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1660990195396,
     "user": {
      "displayName": "Marco Celoria",
      "userId": "03153579951134764980"
     },
     "user_tz": -120
    },
    "id": "BaozE79pT8Jl",
    "outputId": "08ddbe5d-8647-4922-eda7-45926e23560a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1.14583482, -0.56097481, -1.69303519,  0.95920345, -0.57124765,\n",
      "        0.48163571, -0.57639631, -0.30269139]), array([-1.12178754,  0.47756774, -0.57458717, -1.04261051, -1.12178754,\n",
      "        0.47756774,  0.57124765, -0.48163571, -0.00514866, -0.7843271 ]), array([-1.11663889,  1.26189484, -0.56943851, -0.25828342, -1.11663889,\n",
      "        1.26189484,  0.57639631,  0.30269139,  0.00514866,  0.7843271 ])]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b500e2",
   "metadata": {},
   "source": [
    "Let's first define the MultiAgentReplayBuffer class\n",
    "\n",
    "The parameters are related to \n",
    "- the memory size of the replay buffer (max_size)\n",
    "- the number of agents (n_agents)\n",
    "- the dimension of ... (actor_dims) ???\n",
    "- the batch_size\n",
    "- the number possible of actions\n",
    "\n",
    "Other members that are initialize are:\n",
    "- state_memory:     a matrix of mem_size x critic_dims\n",
    "- new_state_memory: a matrix of mem_size x critic_dims\n",
    "- reward_memory:    a matrix of mem_size x n_agents\n",
    "- terminal_memory:  a matrix of mem_size x n_agents\n",
    "\n",
    "So:\n",
    "\n",
    "- state_memory and new_state_memory pertain to the critic and are centralized,\n",
    "- reward_memory and terminal_memory pertain to the actors and are decentralized\n",
    "\n",
    "Upon initialization of the class (\\__init\\__), we also initialize the actor memory (init_actor_memory).\n",
    "\n",
    "Here we initialize the actor_state_memory, actor_new_state_memory and actor_action_memory.\n",
    "\n",
    "These are basically lists of n_agents matrices of sizes mem_size x actor_dims for each actor\n",
    "\n",
    "When we store a transition, we need:\n",
    "- the observations of each agent of the current state (raw\\_obs) that is an array of length n_agents.\n",
    "- the observations of each agent of the new state (raw\\_obs\\_) that is an array of length n_agents.\n",
    "- the actions of each agent (action) that is an array of length n_agents.\n",
    "\n",
    "For each agent agent_idx, we store:\n",
    "- actor_state_memory[agent_idx][index] = raw_obs[agent_idx]\n",
    "- actor_new_state_memory[agent_idx][index] = raw_obs_[agent_idx]\n",
    "- actor_action_memory[agent_idx][index] = action[agent_idx]\n",
    "\n",
    "where the $index \\in [0,mem\\_size]$ identifies the label of the trasition we are storing in the replay buffer\n",
    "\n",
    "Moreover, we have the parameters:\n",
    "- state: an array of length critic_dims\n",
    "- state_: an array of length critic_dims\n",
    "- reward: an array of length n_agents\n",
    "- done: an array of length n_agents (bool)\n",
    "\n",
    "that we store in \n",
    "\n",
    "- state_memory[index][0:critic_dims] = state[0:critic_dims]\n",
    "- new_state_memory[index][0:critic_dims] = state_[0:critic_dims]\n",
    "- reward_memory[index][0:critic_dims] = reward[0:critic_dims]\n",
    "- terminal_memory[index][0:critic_dims] = done[0:critic_dims]\n",
    "\n",
    "Finally, we update the couter, getting ready to store the new transition\n",
    "\n",
    "\n",
    "When we want to sample from the ReplayBuffer, we first check\n",
    "- max_mem = min(self.mem_cntr, self.mem_size) \n",
    "\n",
    "that is \n",
    "\n",
    "- max_mem = mem_cntr, if we have not yet recorded enough trasition to full the mem_size\n",
    "- otherwise, max_mem = mem_size, that is we sample from the whole possible memory\n",
    "\n",
    "From max_mem, we sample a list batch of batch_size indices that we use to sample\n",
    "\n",
    "- states = self.state_memory[batch]\n",
    "- rewards = self.reward_memory[batch]\n",
    "- states_ = self.new_state_memory[batch]\n",
    "- terminal = self.terminal_memory[batch]\n",
    "\n",
    "For each agent, we define the lists actor_states, actor_new_states, actions that contain n_agents arrays, each corresponding to the batch sampled from actor_state_memory, actor_new_state_memory, actor_action_memory of each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1JUNii5AeV-A",
   "metadata": {
    "id": "1JUNii5AeV-A"
   },
   "outputs": [],
   "source": [
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, max_size, critic_dims, actor_dims, n_actions, n_agents, batch_size):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.n_agents = n_agents\n",
    "        self.actor_dims = actor_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.reward_memory = np.zeros((self.mem_size, n_agents))\n",
    "        self.terminal_memory = np.zeros((self.mem_size, n_agents), dtype=bool)\n",
    "\n",
    "        self.init_actor_memory()\n",
    "\n",
    "    def init_actor_memory(self):\n",
    "        self.actor_state_memory = []\n",
    "        self.actor_new_state_memory = []\n",
    "        self.actor_action_memory = []\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            self.actor_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_new_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_action_memory.append(\n",
    "                            np.zeros((self.mem_size, self.n_actions)))\n",
    "\n",
    "\n",
    "    def store_transition(self, raw_obs, state, action, reward, \n",
    "                               raw_obs_, state_, done):\n",
    "        # this introduces a bug: if we fill up the memory capacity and then\n",
    "        # zero out our actor memory, the critic will still have memories to access\n",
    "        # while the actor will have nothing but zeros to sample. Obviously\n",
    "        # not what we intend.\n",
    "        # In reality, there's no problem with just using the same index\n",
    "        # for both the actor and critic states. I'm not sure why I thought\n",
    "        # this was necessary in the first place. Sorry for the confusion!\n",
    "\n",
    "        #if self.mem_cntr % self.mem_size == 0 and self.mem_cntr > 0:\n",
    "        #    self.init_actor_memory()\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.actor_state_memory[agent_idx][index] = raw_obs[agent_idx]\n",
    "            self.actor_new_state_memory[agent_idx][index] = raw_obs_[agent_idx]\n",
    "            self.actor_action_memory[agent_idx][index] = action[agent_idx]\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        actor_states = []\n",
    "        actor_new_states = []\n",
    "        actions = []\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            actor_states.append(self.actor_state_memory[agent_idx][batch])\n",
    "            actor_new_states.append(self.actor_new_state_memory[agent_idx][batch])\n",
    "            actions.append(self.actor_action_memory[agent_idx][batch])\n",
    "\n",
    "        return actor_states, states, actions, rewards, \\\n",
    "               actor_new_states, states_, terminal\n",
    "\n",
    "    def ready(self):\n",
    "        if self.mem_cntr >= self.batch_size:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jN64dj96rObe",
   "metadata": {
    "id": "jN64dj96rObe"
   },
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, \n",
    "                    n_agents, n_actions, name, chkpt_dir):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims+n_agents*n_actions, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.q = nn.Linear(fc2_dims, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(T.cat([state, action], dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, \n",
    "                 n_actions, name, chkpt_dir):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = T.softmax(self.pi(x), dim=1)\n",
    "\n",
    "        return pi\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63dd96",
   "metadata": {},
   "source": [
    "Each agent has 4 Neural Network corresponding to:\n",
    "- actor network\n",
    "- target actor network\n",
    "- critic network\n",
    "- target critic network \n",
    "\n",
    "To choose an action we use the actor network to which we add noise.\n",
    "\n",
    "To update the parameters of the neural networks, we make a copy of the target network parameters and have them slowly track those of the learned networks via “soft updates”, polyak averaging with parater $(1-\\tau) = \\rho$\n",
    "\n",
    "After having constructed the actor_state_dict, we update the target actor network using\n",
    "- self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "After having constructed the critic_state_dict, we update the target actor network using\n",
    "- self.target_critic.load_state_dict(critic_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QrnurHgwrWO2",
   "metadata": {
    "id": "QrnurHgwrWO2"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actor_dims, critic_dims, n_actions, n_agents, agent_idx, chkpt_dir,\n",
    "                    alpha=0.01, beta=0.01, fc1=64, \n",
    "                    fc2=64, gamma=0.95, tau=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_actions = n_actions\n",
    "        self.agent_name = 'agent_%s' % agent_idx\n",
    "        self.actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions, \n",
    "                                  chkpt_dir=chkpt_dir,  name=self.agent_name+'_actor')\n",
    "        self.critic = CriticNetwork(beta, critic_dims, \n",
    "                            fc1, fc2, n_agents, n_actions, \n",
    "                            chkpt_dir=chkpt_dir, name=self.agent_name+'_critic')\n",
    "        self.target_actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions,\n",
    "                                        chkpt_dir=chkpt_dir, \n",
    "                                        name=self.agent_name+'_target_actor')\n",
    "        self.target_critic = CriticNetwork(beta, critic_dims, \n",
    "                                            fc1, fc2, n_agents, n_actions,\n",
    "                                            chkpt_dir=chkpt_dir,\n",
    "                                            name=self.agent_name+'_target_critic')\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        actions = self.actor.forward(state)\n",
    "        noise = T.rand(self.n_actions).to(self.actor.device)\n",
    "        action = actions + noise\n",
    "\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        actor_params = self.actor.named_parameters()\n",
    "\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_actor_state_dict[name].clone()\n",
    "\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_critic_state_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255be948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3atdxmsrq6b",
   "metadata": {
    "id": "f3atdxmsrq6b"
   },
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, actor_dims, critic_dims, n_agents, n_actions, \n",
    "                 scenario='simple',  alpha=0.01, beta=0.01, fc1=64, \n",
    "                 fc2=64, gamma=0.99, tau=0.01, chkpt_dir='tmp/maddpg/'):\n",
    "        self.agents = []\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        chkpt_dir += scenario \n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.agents.append(Agent(actor_dims[agent_idx], critic_dims,  \n",
    "                            n_actions, n_agents, agent_idx, alpha=alpha, beta=beta,\n",
    "                            chkpt_dir=chkpt_dir))\n",
    "\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        for agent in self.agents:\n",
    "            agent.save_models()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        for agent in self.agents:\n",
    "            agent.load_models()\n",
    "\n",
    "    def choose_action(self, raw_obs):\n",
    "        actions = []\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            action = agent.choose_action(raw_obs[agent_idx])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def learn(self, memory):\n",
    "        if not memory.ready():\n",
    "            return\n",
    "\n",
    "        # for agent i=1 to N do\n",
    "           # Sample a random minibatch of S samples from D\n",
    "        actor_states, states, actions, rewards, \\\n",
    "        actor_new_states, states_, dones = memory.sample_buffer()\n",
    "\n",
    "        device = self.agents[0].actor.device\n",
    "        \n",
    "        states = T.tensor(states, dtype=T.float).to(device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(device)\n",
    "        rewards = T.tensor(rewards).to(device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(device)\n",
    "        dones = T.tensor(dones).to(device)\n",
    "\n",
    "        all_agents_new_actions = []\n",
    "        all_agents_new_mu_actions = []\n",
    "        old_agents_actions = []\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            new_states = T.tensor(actor_new_states[agent_idx], \n",
    "                                 dtype=T.float).to(device)\n",
    "\n",
    "            new_pi = agent.target_actor.forward(new_states)\n",
    "\n",
    "            all_agents_new_actions.append(new_pi)\n",
    "            mu_states = T.tensor(actor_states[agent_idx], \n",
    "                                 dtype=T.float).to(device)\n",
    "            pi = agent.actor.forward(mu_states)\n",
    "            all_agents_new_mu_actions.append(pi)\n",
    "            old_agents_actions.append(actions[agent_idx])\n",
    "\n",
    "        new_actions = T.cat([acts for acts in all_agents_new_actions], dim=1)\n",
    "        mu = T.cat([acts for acts in all_agents_new_mu_actions], dim=1)\n",
    "        old_actions = T.cat([acts for acts in old_agents_actions],dim=1)\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            # Get the critic value and the target critic value\n",
    "            critic_value_ = agent.target_critic.forward(states_, new_actions).flatten()\n",
    "            critic_value_[dones[:,0]] = 0.0\n",
    "            critic_value = agent.critic.forward(states, old_actions).flatten()\n",
    "\n",
    "            # To be able to define y_i\n",
    "            target = rewards[:,agent_idx] + agent.gamma*critic_value_\n",
    "            # and the critic loss\n",
    "            critic_loss = F.mse_loss(target, critic_value)\n",
    "            # Update critic by minimizing the loss\n",
    "            agent.critic.optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            agent.critic.optimizer.step()\n",
    "\n",
    "            # Update actor using the sampled policy gradient:\n",
    "            # we can just perform gradient ascent (with respect to policy parameters only)\n",
    "            actor_loss = agent.critic.forward(states, mu).flatten()\n",
    "            actor_loss = -T.mean(actor_loss)\n",
    "            agent.actor.optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            agent.actor.optimizer.step()\n",
    "\n",
    "            # Update the parameters\n",
    "            agent.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1dp_4WRrsEP",
   "metadata": {
    "id": "u1dp_4WRrsEP"
   },
   "outputs": [],
   "source": [
    "def obs_list_to_state_vector(observation):\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WtI2PjRqr4K6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 897540,
     "status": "error",
     "timestamp": 1660992588523,
     "user": {
      "displayName": "Marco Celoria",
      "userId": "03153579951134764980"
     },
     "user_tz": -120
    },
    "id": "WtI2PjRqr4K6",
    "outputId": "76ac3960-c2ec-494c-ac5b-9380573c8969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 500 average score -22.2\n",
      "episode 1000 average score -25.6\n",
      "episode 1500 average score -24.8\n",
      "episode 2000 average score -22.5\n",
      "episode 2500 average score -23.9\n",
      "episode 3000 average score -26.9\n",
      "episode 3500 average score -22.4\n",
      "episode 4000 average score -22.9\n",
      "episode 4500 average score -23.3\n",
      "episode 5000 average score -23.5\n",
      "episode 5500 average score -22.5\n",
      "episode 6000 average score -23.1\n",
      "episode 6500 average score -23.5\n",
      "episode 7000 average score -21.0\n",
      "episode 7500 average score -19.5\n",
      "episode 8000 average score -19.9\n",
      "episode 8500 average score -22.5\n",
      "episode 9000 average score -23.6\n",
      "episode 9500 average score -21.4\n",
      "episode 10000 average score -22.7\n",
      "episode 10500 average score -21.5\n",
      "episode 11000 average score -23.9\n",
      "episode 11500 average score -21.4\n",
      "episode 12000 average score -23.1\n",
      "episode 12500 average score -22.8\n",
      "episode 13000 average score -24.0\n",
      "episode 13500 average score -25.8\n",
      "episode 14000 average score -23.4\n",
      "episode 14500 average score -21.8\n",
      "episode 15000 average score -19.8\n",
      "episode 15500 average score -18.7\n",
      "episode 16000 average score -20.1\n",
      "episode 16500 average score -18.2\n",
      "episode 17000 average score -17.4\n",
      "episode 17500 average score -21.7\n",
      "episode 18000 average score -17.0\n",
      "episode 18500 average score -13.4\n",
      "episode 19000 average score -5.6\n",
      "episode 19500 average score -6.5\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode 20000 average score 5.4\n",
      "episode 20500 average score 3.1\n",
      "episode 21000 average score 3.7\n",
      "episode 21500 average score 4.9\n",
      "... saving checkpoint ...\n",
      "episode 22000 average score 4.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-086a8b6eedd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#time.sleep(0.1) # to slow down the action for the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaddpg_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mobs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0cb7897d6190>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, raw_obs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dcef3a535ba8>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#scenario = 'simple'\n",
    "os.makedirs(\"tmp/maddpg/simple_adversary/\", exist_ok=True)\n",
    "scenario = 'simple_adversary'\n",
    "env = make_env(scenario)\n",
    "n_agents = env.n\n",
    "actor_dims = []\n",
    "for i in range(n_agents):\n",
    "    actor_dims.append(env.observation_space[i].shape[0])\n",
    "critic_dims = sum(actor_dims)\n",
    "\n",
    "# action space is a list of arrays, assume each agent has same action space\n",
    "n_actions = env.action_space[0].n\n",
    "maddpg_agents = MADDPG(actor_dims, critic_dims, n_agents, n_actions, \n",
    "                           fc1=64, fc2=64,  \n",
    "                           alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                           chkpt_dir='tmp/maddpg/')#\n",
    "\n",
    "memory = MultiAgentReplayBuffer(1000000, critic_dims, actor_dims, n_actions, n_agents, batch_size=1024)\n",
    "\n",
    "PRINT_INTERVAL = 500\n",
    "N_GAMES = 50000\n",
    "MAX_STEPS = 25\n",
    "total_steps = 0\n",
    "score_history = []\n",
    "evaluate = False\n",
    "best_score = 0\n",
    "\n",
    "if evaluate:\n",
    "    maddpg_agents.load_checkpoint()\n",
    "\n",
    "for i in range(N_GAMES):\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    done = [False]*n_agents\n",
    "    episode_step = 0\n",
    "    while not any(done):\n",
    "        if evaluate:\n",
    "            env.render()\n",
    "            #time.sleep(0.1) # to slow down the action for the video\n",
    "        actions = maddpg_agents.choose_action(obs)\n",
    "        obs_, reward, done, info = env.step(actions)\n",
    "\n",
    "        state = obs_list_to_state_vector(obs)\n",
    "        state_ = obs_list_to_state_vector(obs_)\n",
    "\n",
    "        if episode_step >= MAX_STEPS:\n",
    "            done = [True]*n_agents\n",
    "\n",
    "        memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "\n",
    "        if total_steps % 100 == 0 and not evaluate:\n",
    "            maddpg_agents.learn(memory)\n",
    "\n",
    "        obs = obs_\n",
    "\n",
    "        score += sum(reward)\n",
    "        total_steps += 1\n",
    "        episode_step += 1\n",
    "\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    if not evaluate:\n",
    "        if avg_score > best_score:\n",
    "            maddpg_agents.save_checkpoint()\n",
    "            best_score = avg_score\n",
    "    if i % PRINT_INTERVAL == 0 and i > 0:\n",
    "        print('episode', i, 'average score {:.1f}'.format(avg_score))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of mymaddpg.ipynb",
   "provenance": [
    {
     "file_id": "1oN9o5I6iMB3TGSt2UuHtwVkd1jTDFyiG",
     "timestamp": 1660992597434
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
